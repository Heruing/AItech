## 경사하강법 - 매운맛

- **선형회귀 계수**

  - `np.linalg.pinv`를 이용한 선형회귀식

  - 선형회귀식은 데이터를 선형모델로 해석

    

- **경사하강법**

  - 선형회귀의 목적식은 ∥y − Xβ∥<sub>2</sub>이고 이를 최소화하는 β를 구해야 함
  - 목적식을 최소화하는 β를 구하는 경사하강법 알고리즘
    ![beta_algo](README.assets/beta_algo.PNG)
  - 경사하강법은 미분가능하고 볼록한 함수에 대해서, 적절한 학습률과 횟수를 선택했을 때 수렴이 보장
  - 선형회귀의 경우 목적식 ∥y − Xβ∥<sub>2</sub>은 β에 대해 볼록함수라 수렴이 보장
  - 비선형회귀 문제의 경우 목적식이 볼록하지 않을 수 있어 수렴이 보장되지 않음


- **확률적 경사하강법**(stochastic gradient descent, SGD)
  - 모든 데이터를 사용해서 업데이트 하는 대신 한개 또는 일부 활용하여 업데이트
  - 볼록하지 않은 목적식은 SGD를 통해 최적화
  - 딥러닝의 경우 SGD가 경사하강법보다 실정적으로 더 낫다고 검증
  - SGD는 데이터의 일부를 가지고 업데이트하기 때문에 연산자원을 조금 더 효율적으로 활용
- **SGD의 원리**
  - 미니배치 연산
    - 경사하강법은 전체 데이터 D = (X, y)를 가지고 그레디언트 벡터 ∇<sub>θ</sub>L(D, θ)를 계산
    - SGD는 미니배치 D<sub>(b)</sub> = (X<sub>(b)</sub>, y<sub>(b)</sub>) ⊂ D 를 가지고 그레디언트 벡터를 계산
    - 미니배치는 확률적으로 선택하므로 목적식 모양이 변화
    - SGD는 블록이 아닌 목적식에서도 사용가능하므로 머신러닝 학습에 효율적
  - 하드웨어
    - 경사하강법처럼 모든 데이터를 업로드하면 Out-of-memory 발생
